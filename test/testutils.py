from __future__ import annotations
import os
import sys
import json
import time
import signal
import psutil
import threading
from pathlib import Path
from collections import deque
from typing import Optional, Dict, Callable, Any
from contextlib import contextmanager
from datetime import datetime

KONECT_PATH = "/auto/datasets/graphs/dynamic_konect_project_datasets/"
PROJECT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
INFO = os.path.join(PROJECT_DIR, "datasets-info")
KONECT_INFO = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "datasets-info"))

def get_all_datasets():
    """
    Ð¡reate dict with all datasets in test directory.
    """
    base_dir = os.path.join(os.path.dirname(__file__), "graphs", "small")
    datasets = {}
    if os.path.isdir(base_dir):
        for name in os.listdir(base_dir):
            path = os.path.join(base_dir, name)
            if os.path.isdir(path):
                datasets[name] = base_dir
    return datasets

def load_konect_names(all_json_path: Path) -> set[str]:
    if not all_json_path.exists():
        return set()
    data = json.loads(all_json_path.read_text(encoding="utf-8"))
    if isinstance(data, dict):
        return set(data.keys())
    if isinstance(data, list):
        return {d["name"] for d in data if isinstance(d, dict) and "name" in d}
    return set()

def filter_datasets_by_node_count(json_path: Path, max_nodes: int):
    with json_path.open("r", encoding="utf-8") as f:
        data = json.load(f)

    filtered = {name: info for name, info in data.items() if info.get("n", 0) < max_nodes}
    return filtered


class TimeoutException(Exception):
    """Ð˜ÑÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð¿Ñ€ÐµÐ²Ñ‹ÑˆÐµÐ½Ð¸Ñ Ñ‚Ð°Ð¹Ð¼-Ð°ÑƒÑ‚Ð° Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ"""
    pass


class ResourceMonitor:
    """
    ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð² Ð¿Ñ€Ð¾Ñ†ÐµÑÑÐ° Ð² Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸
    
    ÐžÑ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°ÐµÑ‚ CPU, RAM Ñ Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼ Ð²Ñ‹Ð²Ð¾Ð´Ð¾Ð¼ Ð² ÐºÐ¾Ð½ÑÐ¾Ð»ÑŒ.
    Ð Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð² Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾Ð¼ Ñ„Ð¾Ð½Ð¾Ð²Ð¾Ð¼ Ð¿Ð¾Ñ‚Ð¾ÐºÐµ.
    
    Args:
        interval: Ð˜Ð½Ñ‚ÐµÑ€Ð²Ð°Ð» Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð¼ÐµÑ‚Ñ€Ð¸Ðº Ð² ÑÐµÐºÑƒÐ½Ð´Ð°Ñ…
        history_size: Ð Ð°Ð·Ð¼ÐµÑ€ Ð±ÑƒÑ„ÐµÑ€Ð° Ð´Ð»Ñ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ð¼ÐµÑ‚Ñ€Ð¸Ðº
        
    Example:
        monitor = ResourceMonitor(interval=0.5)
        monitor.start()
        # ... Ð²Ð°Ñˆ ÐºÐ¾Ð´ ...
        monitor.stop()
        monitor.print_summary()
    """
    
    def __init__(self, interval: float = 0.5, history_size: int = 100):
        self.interval = interval
        self.history_size = history_size
        self.running = False
        self.thread: Optional[threading.Thread] = None
        self.process = psutil.Process()
        
        # Ð˜ÑÑ‚Ð¾Ñ€Ð¸Ñ Ð¼ÐµÑ‚Ñ€Ð¸Ðº
        self.cpu_history = deque(maxlen=history_size)
        self.mem_history = deque(maxlen=history_size)
        self.timestamps = deque(maxlen=history_size)
        
    def _monitor_loop(self):
        """Ð¤Ð¾Ð½Ð¾Ð²Ñ‹Ð¹ Ñ†Ð¸ÐºÐ» Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³Ð°"""
        start_time = time.time()
        
        # ÐŸÐµÑ€Ð²Ñ‹Ð¹ Ð²Ñ‹Ð·Ð¾Ð² Ð´Ð»Ñ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ cpu_percent
        self.process.cpu_percent(interval=None)
        
        while self.running:
            try:
                # CPU usage (%)
                cpu_percent = self.process.cpu_percent(interval=None)
                
                # Memory usage (MB)
                mem_info = self.process.memory_info()
                mem_mb = mem_info.rss / (1024 * 1024)
                
                # Timestamp
                elapsed = time.time() - start_time
                
                # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸
                self.cpu_history.append(cpu_percent)
                self.mem_history.append(mem_mb)
                self.timestamps.append(elapsed)
                
                # Ð”Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð²Ñ‹Ð²Ð¾Ð´
                self._print_stats(cpu_percent, mem_mb, elapsed)
                
                time.sleep(self.interval)
                
            except psutil.NoSuchProcess:
                break
            except Exception as e:
                print(f"\nâš ï¸  Monitor error: {e}")
                break
                
    def _print_stats(self, cpu: float, mem: float, elapsed: float):
        """Ð”Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð²Ñ‹Ð²Ð¾Ð´ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ð² Ð¾Ð´Ð½Ñƒ ÑÑ‚Ñ€Ð¾ÐºÑƒ"""
        sys.stdout.write('\r' + ' ' * 100 + '\r')
    
        # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ CPU Ð¿Ð¾ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ñƒ ÑÐ´ÐµÑ€
        num_cores = psutil.cpu_count()
        cpu_normalized = cpu / num_cores if num_cores else cpu
        
        peak_mem = max(self.mem_history) if self.mem_history else mem
        stats = (
            f"â±ï¸  {elapsed:6.1f}s | "
            f"CPU: {cpu_normalized:5.1f}% ({cpu:6.1f}% total) | "
            f"RAM: {mem:7.1f} MB | "
            f"Peak RAM: {peak_mem:7.1f} MB"
        )
        sys.stdout.write(stats)
        sys.stdout.flush()
        
    def start(self):
        """Ð—Ð°Ð¿ÑƒÑÐº Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³Ð° Ð² Ñ„Ð¾Ð½Ð¾Ð²Ð¾Ð¼ Ð¿Ð¾Ñ‚Ð¾ÐºÐµ"""
        if not self.running:
            self.running = True
            self.thread = threading.Thread(target=self._monitor_loop, daemon=True)
            self.thread.start()
            
    def stop(self):
        """ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³Ð°"""
        if self.running:
            self.running = False
            if self.thread:
                self.thread.join(timeout=2)
            print()  # ÐŸÐµÑ€ÐµÐ²Ð¾Ð´ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð¿Ð¾ÑÐ»Ðµ Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð²Ñ‹Ð²Ð¾Ð´Ð°
            
    def get_summary(self) -> Dict[str, float]:
        """
        ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ð¾Ð¹ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸
        
        Returns:
            Ð¡Ð»Ð¾Ð²Ð°Ñ€ÑŒ Ñ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ°Ð¼Ð¸: cpu_avg, cpu_max, mem_avg, mem_max, duration
        """
        if not self.cpu_history:
            return {}
            
        return {
            'cpu_avg': sum(self.cpu_history) / len(self.cpu_history),
            'cpu_max': max(self.cpu_history),
            'mem_avg': sum(self.mem_history) / len(self.mem_history),
            'mem_max': max(self.mem_history),
            'duration': self.timestamps[-1] if self.timestamps else 0
        }
        
    def print_summary(self):
        """Ð’Ñ‹Ð²Ð¾Ð´ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²Ð¾Ð¹ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ð² ÐºÐ¾Ð½ÑÐ¾Ð»ÑŒ"""
        summary = self.get_summary()
        if summary:
            print(f"\n{'='*60}")
            print(f"Resource Usage Summary:")
            print(f"  Duration:     {summary['duration']:.2f} s")
            print(f"  CPU Average:  {summary['cpu_avg']:.2f}%")
            print(f"  CPU Peak:     {summary['cpu_max']:.2f}%")
            print(f"  RAM Average:  {summary['mem_avg']:.2f} MB")
            print(f"  RAM Peak:     {summary['mem_max']:.2f} MB")
            print(f"{'='*60}")


class ResourceMonitorWithGPU(ResourceMonitor):
    """
    Ð Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ‹Ð¹ Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹ GPU
    
    Ð¢Ñ€ÐµÐ±ÑƒÐµÑ‚ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ð¾Ð³Ð¾ PyTorch Ñ CUDA.
    
    Args:
        interval: Ð˜Ð½Ñ‚ÐµÑ€Ð²Ð°Ð» Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð¼ÐµÑ‚Ñ€Ð¸Ðº
        history_size: Ð Ð°Ð·Ð¼ÐµÑ€ Ð±ÑƒÑ„ÐµÑ€Ð° Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸
        track_gpu: Ð’ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ GPU (ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾)
    """
    
    def __init__(self, interval: float = 0.5, history_size: int = 100, track_gpu: bool = True):
        super().__init__(interval, history_size)
        self.track_gpu = track_gpu
        self.gpu_available = False
        
        if track_gpu:
            try:
                import torch
                self.gpu_available = torch.cuda.is_available()
                if self.gpu_available:
                    self.gpu_mem_history = deque(maxlen=history_size)
                    self.torch = torch
            except ImportError:
                pass
                
    def _monitor_loop(self):
        """Ð Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ‹Ð¹ Ñ†Ð¸ÐºÐ» Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³Ð° Ñ GPU"""
        start_time = time.time()
        self.process.cpu_percent(interval=None)
        
        while self.running:
            try:
                cpu_percent = self.process.cpu_percent(interval=None)
                mem_info = self.process.memory_info()
                mem_mb = mem_info.rss / (1024 * 1024)
                elapsed = time.time() - start_time
                
                # GPU Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³
                gpu_mem_mb = 0
                if self.gpu_available:
                    gpu_mem_mb = self.torch.cuda.memory_allocated() / (1024 * 1024)
                    self.gpu_mem_history.append(gpu_mem_mb)
                
                self.cpu_history.append(cpu_percent)
                self.mem_history.append(mem_mb)
                self.timestamps.append(elapsed)
                
                self._print_stats_gpu(cpu_percent, mem_mb, gpu_mem_mb, elapsed)
                
                time.sleep(self.interval)
                
            except (psutil.NoSuchProcess, RuntimeError):
                break
                
    def _print_stats_gpu(self, cpu: float, mem: float, gpu_mem: float, elapsed: float):
        """Ð Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ‹Ð¹ Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð²Ñ‹Ð²Ð¾Ð´ Ñ GPU"""
        sys.stdout.write('\r' + ' ' * 120 + '\r')
        
        peak_mem = max(self.mem_history) if self.mem_history else mem
        stats = (
            f"â±ï¸  {elapsed:6.1f}s | "
            f"CPU: {cpu:5.1f}% | "
            f"RAM: {mem:7.1f} MB (peak: {peak_mem:7.1f})"
        )
        
        if self.gpu_available:
            peak_gpu = max(self.gpu_mem_history) if self.gpu_mem_history else gpu_mem
            stats += f" | GPU: {gpu_mem:7.1f} MB (peak: {peak_gpu:7.1f})"
            
        sys.stdout.write(stats)
        sys.stdout.flush()
        
    def get_summary(self) -> Dict[str, float]:
        if not self.cpu_history:
            return {}
        cpu_total_avg = sum(self.cpu_history) / len(self.cpu_history)
        cpu_total_max = max(self.cpu_history)
        mem_avg = sum(self.mem_history) / len(self.mem_history)
        mem_max = max(self.mem_history)
        duration = self.timestamps[-1] if self.timestamps else 0
        return {
            'cpu_total_avg': cpu_total_avg,
            'cpu_total_max': cpu_total_max,
            'cpu_per_core_avg': cpu_total_avg / self.num_cores,
            'cpu_per_core_max': cpu_total_max / self.num_cores,
            'mem_avg': mem_avg,
            'mem_max': mem_max,
            'duration': duration,
            'num_cores': self.num_cores
        }

    def print_summary(self):
        summary = self.get_summary()
        if summary:
            print(f"\n{'='*60}")
            print("Resource Usage Summary:")
            print(f"  Duration:     {summary['duration']:.2f} s")
            print(f"  CPU Average:  {summary['cpu_per_core_avg']:.2f}%/core ({summary['cpu_total_avg']:.2f}% total)")
            print(f"  CPU Peak:     {summary['cpu_per_core_max']:.2f}%/core ({summary['cpu_total_max']:.2f}% total)")
            print(f"  RAM Average:  {summary['mem_avg']:.2f} MB")
            print(f"  RAM Peak:     {summary['mem_max']:.2f} MB")
            print(f"  Cores:        {summary['num_cores']}")
            print(f"{'='*60}")



class ResourceMonitorWithLogging(ResourceMonitorWithGPU):
    """
    ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸ÐµÐ¼ Ð»Ð¾Ð³Ð¾Ð² Ð² JSON
    
    Args:
        log_file: ÐŸÑƒÑ‚ÑŒ Ðº Ñ„Ð°Ð¹Ð»Ñƒ Ð»Ð¾Ð³Ð¾Ð² (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ÑÑ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸)
        interval: Ð˜Ð½Ñ‚ÐµÑ€Ð²Ð°Ð» Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ
        history_size: Ð Ð°Ð·Ð¼ÐµÑ€ Ð±ÑƒÑ„ÐµÑ€Ð°
        track_gpu: ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ñ‚ÑŒ GPU
    """
    
    def __init__(
        self, 
        log_file: Optional[str] = None, 
        interval: float = 0.5,
        history_size: int = 100,
        track_gpu: bool = True
    ):
        super().__init__(interval, history_size, track_gpu)
        
        if log_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            log_file = f"resource_log_{timestamp}.json"
            
        self.log_file = Path(log_file)
        self.logs = []
        
    def _monitor_loop(self):
        """ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸ÐµÐ¼ Ð² Ð»Ð¾Ð³"""
        start_time = time.time()
        self.process.cpu_percent(interval=None)
        
        while self.running:
            try:
                cpu_percent = self.process.cpu_percent(interval=None)
                mem_info = self.process.memory_info()
                mem_mb = mem_info.rss / (1024 * 1024)
                elapsed = time.time() - start_time
                
                gpu_mem_mb = 0
                if self.gpu_available:
                    gpu_mem_mb = self.torch.cuda.memory_allocated() / (1024 * 1024)
                    self.gpu_mem_history.append(gpu_mem_mb)
                
                # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð² Ð»Ð¾Ð³
                log_entry = {
                    'timestamp': elapsed,
                    'cpu_percent': cpu_percent,
                    'mem_mb': mem_mb,
                    'gpu_mem_mb': gpu_mem_mb if self.gpu_available else None
                }
                self.logs.append(log_entry)
                
                self.cpu_history.append(cpu_percent)
                self.mem_history.append(mem_mb)
                self.timestamps.append(elapsed)
                
                self._print_stats_gpu(cpu_percent, mem_mb, gpu_mem_mb, elapsed)
                
                time.sleep(self.interval)
                
            except (psutil.NoSuchProcess, RuntimeError):
                break
                
    def save_logs(self):
        """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð»Ð¾Ð³Ð¾Ð² Ð² JSON Ñ„Ð°Ð¹Ð»"""
        log_data = {
            'logs': self.logs,
            'summary': self.get_summary(),
            'metadata': {
                'start_time': datetime.now().isoformat(),
                'interval': self.interval,
                'history_size': self.history_size,
                'gpu_tracked': self.gpu_available
            }
        }
        
        with open(self.log_file, 'w') as f:
            json.dump(log_data, f, indent=2)
            
        print(f"\nðŸ“Š Logs saved to: {self.log_file}")
        
    def stop(self):
        """ÐžÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° Ñ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸ÐµÐ¼"""
        super().stop()
        if self.logs:
            self.save_logs()


def with_timeout(seconds: int):
    """
    Ð”ÐµÐºÐ¾Ñ€Ð°Ñ‚Ð¾Ñ€ Ð´Ð»Ñ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸ Ñ‚Ð°Ð¹Ð¼-Ð°ÑƒÑ‚Ð° Ð½Ð° Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸
    
    Args:
        seconds: ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð²Ñ€ÐµÐ¼Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð² ÑÐµÐºÑƒÐ½Ð´Ð°Ñ…
        
    Raises:
        TimeoutException: Ð•ÑÐ»Ð¸ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð½Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐ¸Ð»Ð°ÑÑŒ Ð·Ð° Ð¾Ñ‚Ð²ÐµÐ´Ñ‘Ð½Ð½Ð¾Ðµ Ð²Ñ€ÐµÐ¼Ñ
        
    Example:
        @with_timeout(300)
        def long_running_function():
            # ÐºÐ¾Ð´, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¼Ð¾Ð¶ÐµÑ‚ Ð·Ð°Ð²Ð¸ÑÐ½ÑƒÑ‚ÑŒ
            pass
    """
    def decorator(func: Callable) -> Callable:
        def wrapper(*args, **kwargs) -> Any:
            def _timeout_handler(signum, frame):
                raise TimeoutException(
                    f"Function '{func.__name__}' exceeded timeout of {seconds}s"
                )
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÑÑ‚Ð°Ñ€Ñ‹Ð¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸Ðº
            old_handler = signal.signal(signal.SIGALRM, _timeout_handler)
            signal.alarm(seconds)
            
            try:
                result = func(*args, **kwargs)
            finally:
                signal.alarm(0)
                signal.signal(signal.SIGALRM, old_handler)
                
            return result
        return wrapper
    return decorator


@contextmanager
def monitored_execution(
    interval: float = 0.5,
    track_gpu: bool = False,
    save_logs: bool = False,
    log_file: Optional[str] = None,
    print_summary: bool = True
):
    """
    ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð½Ñ‹Ð¹ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€ Ð´Ð»Ñ Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³Ð° Ð±Ð»Ð¾ÐºÐ° ÐºÐ¾Ð´Ð°
    
    Args:
        interval: Ð˜Ð½Ñ‚ÐµÑ€Ð²Ð°Ð» Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð¼ÐµÑ‚Ñ€Ð¸Ðº
        track_gpu: ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ñ‚ÑŒ GPU
        save_logs: Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÑ‚ÑŒ Ð»Ð¾Ð³Ð¸ Ð² Ñ„Ð°Ð¹Ð»
        log_file: ÐŸÑƒÑ‚ÑŒ Ðº Ñ„Ð°Ð¹Ð»Ñƒ Ð»Ð¾Ð³Ð¾Ð²
        print_summary: ÐŸÐµÑ‡Ð°Ñ‚Ð°Ñ‚ÑŒ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²ÑƒÑŽ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ
        
    Yields:
        ResourceMonitor: ÐžÐ±ÑŠÐµÐºÑ‚ Ð¼Ð¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³Ð°
        
    Example:
        with monitored_execution(track_gpu=True) as monitor:
            # Ð²Ð°Ñˆ ÐºÐ¾Ð´
            heavy_computation()
            
        # ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð²Ñ‹Ð²ÐµÐ´ÐµÑ‚ÑÑ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸
    """
    if save_logs:
        monitor = ResourceMonitorWithLogging(
            log_file=log_file,
            interval=interval,
            track_gpu=track_gpu
        )
    elif track_gpu:
        monitor = ResourceMonitorWithGPU(interval=interval, track_gpu=True)
    else:
        monitor = ResourceMonitor(interval=interval)
    
    monitor.start()
    
    try:
        yield monitor
    finally:
        monitor.stop()
        if print_summary:
            monitor.print_summary()


def measure_time(func: Callable) -> Callable:
    """
    Ð”ÐµÐºÐ¾Ñ€Ð°Ñ‚Ð¾Ñ€ Ð´Ð»Ñ Ð¸Ð·Ð¼ÐµÑ€ÐµÐ½Ð¸Ñ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸
    
    Example:
        @measure_time
        def my_function():
            pass
    """
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        elapsed = time.time() - start
        print(f"\nâ±ï¸  {func.__name__} took {elapsed:.2f}s")
        return result
    return wrapper

def main() -> None:
    test_dir = Path(__file__).resolve().parent
    repo_root = test_dir.parent
    print(repo_root)
    all_json = repo_root / "datasets-info" / "all.json"
    small_root = test_dir / "graphs" / "small"
    out_path = test_dir / "dataset_paths.json"

    MAX_NODES = 10000

    filtered_datasets = filter_datasets_by_node_count(all_json, MAX_NODES)

    mapping: dict[str, str] = {}
    for name in filtered_datasets:
        mapping[name] = KONECT_PATH

    if small_root.exists():
        for p in small_root.iterdir():
            if p.is_dir():
                mapping[p.name] = str(small_root)

    out_path.write_text(json.dumps(mapping, ensure_ascii=False, indent=2), encoding="utf-8")

if __name__ == "__main__":
    main()