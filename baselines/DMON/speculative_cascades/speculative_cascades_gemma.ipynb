{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2025 Google LLC.\n",
        "\n",
        "SPDX-License-Identifier: Apache-2.0"
      ],
      "metadata": {
        "id": "V7tlfFT7oWW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Speculative Cascades between Gemma models\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "cellView": "form",
        "id": "I3_Npu_5oniG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This colab is based on the <a href=\"https://gemma-llm.readthedocs.io/en/latest/colab_sampling.html\">Sampling example</a> tutorial provided with Gemma and forks of the <a href=\"https://github.com/google-deepmind/gemma/blob/2925236fb0d1ff07e4d8abc96f7ff2fe3d9b1ee3/gemma/sampler.py#L81\">Sampler</a> class in their open-sourced\n",
        "<a href=\"https://github.com/google-deepmind/gemma\">codebase</a>. The purpose of this colab is to illustrate cost-quality trade-offs using speculative cascades. We therefore provides a simple implementation, where the drafter generates only one draft token at a time, and the verifier is run for one step to either accept the token, or reject and replace it. In practice, one usually runs the drafter for multiple steps, and runs the verifier in parallel scoring mode to verify the draft tokens.\n",
        "\n",
        "**Acknowledgement:** We thank Ananda Theertha Suresh for invaluable help in writing this colab.\n",
        "\n"
      ],
      "metadata": {
        "id": "8ChYtAL6mQ9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gemma"
      ],
      "metadata": {
        "id": "vCiKFifi0cHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections.abc import Callable, Sequence\n",
        "import dataclasses\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import chex\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from gemma import gm\n",
        "from gemma import modules\n",
        "from gemma.deprecated import transformer as transformer_lib\n",
        "from gemma.deprecated import params as params_lib"
      ],
      "metadata": {
        "id": "aBZrNigWvIIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6-fczEtmLgy"
      },
      "cell_type": "markdown",
      "source": [
        "By default, Jax do not utilize the full GPU memory, but this can be overwritten. See [GPU memory allocation](https://docs.jax.dev/en/latest/gpu_memory_allocation.html):"
      ]
    },
    {
      "metadata": {
        "id": "AaK17GWo3v5Y"
      },
      "cell_type": "code",
      "source": [
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom speculative sampler"
      ],
      "metadata": {
        "id": "FLpBbHY7uiNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generic function to sample next token"
      ],
      "metadata": {
        "id": "QyhGYKBaoOgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_PROBS = 1e-10\n",
        "\n",
        "\n",
        "def sample_next_token(\n",
        "    logits_small: jnp.ndarray,\n",
        "    logits_large: jnp.ndarray,\n",
        "    acceptance_prob_fn: Callable[[jnp.ndarray, jnp.ndarray, jnp.ndarray,\n",
        "                                  jnp.ndarray, jnp.ndarray], jnp.ndarray],\n",
        "    residual_distribution_fn: Callable[[jnp.ndarray, jnp.ndarray, jnp.ndarray,\n",
        "                                        jnp.ndarray, jnp.ndarray], jnp.ndarray],\n",
        "    rng: jnp.ndarray,\n",
        "    temperature: float = 1.0) -> tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
        "  \"\"\"Generic function for sampling the next token from small and large model logits.\"\"\"\n",
        "  # Random generator keys.\n",
        "  rng, rng_small, rng_acceptance, rng_residual = jax.random.split(rng, 4)\n",
        "\n",
        "  # Normalize logits to avoid overflows.\n",
        "  logits_small = jax.nn.log_softmax(logits_small)\n",
        "  logits_large = jax.nn.log_softmax(logits_large)\n",
        "\n",
        "  # Probs without temperature scaling.\n",
        "  probs_small_unscaled = jax.nn.softmax(logits_small, axis=-1)\n",
        "  probs_large_unscaled = jax.nn.softmax(logits_large, axis=-1)\n",
        "\n",
        "  if temperature == 1.0:\n",
        "    probs_small = probs_small_unscaled\n",
        "    probs_large = probs_large_unscaled\n",
        "  elif temperature > 0.0:\n",
        "    probs_small = jax.nn.softmax(logits_small / temperature, axis=-1)\n",
        "    probs_large = jax.nn.softmax(logits_large / temperature, axis=-1)\n",
        "  else:\n",
        "    # For temperature = 0, we compute a one-hot encoding for the argmax token.\n",
        "    probs_small = jax.nn.one_hot(\n",
        "        jnp.argmax(logits_small, axis=-1),\n",
        "        logits_small.shape[-1],\n",
        "        axis=-1)\n",
        "    probs_large = jax.nn.one_hot(\n",
        "        jnp.argmax(logits_large, axis=-1),\n",
        "        logits_large.shape[-1],\n",
        "        axis=-1)\n",
        "\n",
        "  # Sample from small model.\n",
        "  token_small = jax.random.categorical(\n",
        "      rng_small, jnp.log(probs_small), axis=-1)  # [B, 1]\n",
        "\n",
        "  # Should we accept the token?\n",
        "  acceptance_prob = acceptance_prob_fn(\n",
        "      probs_small,\n",
        "      probs_large,\n",
        "      probs_small_unscaled,\n",
        "      probs_large_unscaled,\n",
        "      token_small)   # [B, 1]\n",
        "  is_token_accepted = jax.random.bernoulli(\n",
        "      rng_acceptance, acceptance_prob)  # [B, 1]\n",
        "\n",
        "  # In the event of a rejection, sample from a residual distribution.\n",
        "  probs_residual = residual_distribution_fn(\n",
        "      probs_small,\n",
        "      probs_large,\n",
        "      probs_small_unscaled,\n",
        "      probs_large_unscaled,\n",
        "      token_small)  # [B, 1]\n",
        "  logits_residual = jnp.log(probs_residual)  # [B, 1]\n",
        "  token_residual = jax.random.categorical(\n",
        "      rng_residual, logits_residual, axis=-1)  # [B, 1]\n",
        "\n",
        "  # Return the next token.\n",
        "  next_token = jnp.where(is_token_accepted, token_small, token_residual)\n",
        "  return next_token, rng, is_token_accepted"
      ],
      "metadata": {
        "id": "OCcUgJDXVPYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generic sampler class"
      ],
      "metadata": {
        "id": "iwQuY2tOoSzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _compute_attention_masks(\n",
        "    time_step: jax.Array, seq_len: int, input_mask: jax.Array\n",
        ") -> jax.Array:\n",
        "  \"\"\"Computes causal attention mask.\"\"\"\n",
        "  bsz = input_mask.shape[0]\n",
        "  batch_time_step = jnp.full((bsz, 1), time_step, dtype=jnp.uint32)\n",
        "  causal_padding = jnp.greater(\n",
        "      jnp.expand_dims(jnp.arange(seq_len), 0), batch_time_step\n",
        "  )\n",
        "  max_seq_len = min(input_mask.shape[-1], seq_len)\n",
        "  input_mask = jax.lax.dynamic_slice(\n",
        "      input_mask,\n",
        "      (0, jnp.maximum(time_step - seq_len + 1, 0)),\n",
        "      (bsz, max_seq_len),\n",
        "  )\n",
        "  input_mask = (\n",
        "      jnp.zeros((bsz, seq_len), dtype=jnp.bool_)\n",
        "      .at[:, :max_seq_len]\n",
        "      .set(input_mask)\n",
        "  )\n",
        "\n",
        "  causal_padding = jnp.logical_or(causal_padding, input_mask)\n",
        "  attention_mask = causal_padding[:, jnp.newaxis, :].astype(jnp.bool_)\n",
        "\n",
        "  return ~attention_mask"
      ],
      "metadata": {
        "id": "SJno35NM0crt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@chex.dataclass\n",
        "class _SamplingState:\n",
        "  \"\"\"Internal sampling state.\"\"\"\n",
        "\n",
        "  # Decoding step.\n",
        "  decoding_step: jnp.int32\n",
        "\n",
        "  # Number of tokens in the prompt.\n",
        "  num_input_tokens: jnp.ndarray  # [B]\n",
        "\n",
        "  # Fixed-size buffer for accumulating the output tokens.\n",
        "  token_buffer: jnp.ndarray  # [B, L]\n",
        "\n",
        "  # Position indices, based on ignoring pad tokens.\n",
        "  positions: jnp.ndarray  # [B, L]\n",
        "\n",
        "  # Model state for conditioning the model on autoregressively.\n",
        "  small_cache: dict[str, modules.LayerCache]\n",
        "  large_cache: dict[str, modules.LayerCache]\n",
        "\n",
        "  # Is decoding done on the given sequence?\n",
        "  done: jnp.ndarray  # [B]\n",
        "\n",
        "  # Total sampling steps (including the prompt).\n",
        "  total_sampling_steps: int\n",
        "\n",
        "  # rng key for sampling\n",
        "  rng: jnp.ndarray\n",
        "\n",
        "  # Booleans indicating if each token was accepted.\n",
        "  tokens_accepted: jnp.ndarray\n",
        "\n",
        "  # Fixed-size buffer for accumulating the output logits.\n",
        "  logits_buffer: jnp.ndarray | None = None  # [B, L, V]\n",
        "\n",
        "  # List of tokens that are forbidden to be generated.\n",
        "  forbidden_token_ids: Sequence[int] | None = None"
      ],
      "metadata": {
        "id": "JpxdtuWhx8bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclasses.dataclass\n",
        "class SamplerOutput:\n",
        "\n",
        "  # Decoded samples from the model.\n",
        "  text: list[str]\n",
        "\n",
        "  # Per-step logits used during sampling.\n",
        "  logits: list[list[float]]\n",
        "\n",
        "  # Tokens corresponding to the generated samples.\n",
        "  tokens: list[list[int]]\n",
        "\n",
        "  # Total tokens accepted.\n",
        "  tokens_accepted: list[int]"
      ],
      "metadata": {
        "id": "o7wb7GmfyAcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampler:\n",
        "  \"\"\"Sampler for gemma transformer.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      small_transformer: transformer_lib.Transformer,\n",
        "      large_transformer: transformer_lib.Transformer,\n",
        "      tokenizer: gm.text.Tokenizer,\n",
        "      small_params: params_lib.Params,\n",
        "      large_params: params_lib.Params,\n",
        "      acceptance_prob_fn: Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray],\n",
        "      residual_distribution_fn: Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray],\n",
        "      temperature: float = 1.0,\n",
        "      cache_length: int = 1024,\n",
        "  ):\n",
        "    \"\"\"Initializes a sampler for a Gemma model.\n",
        "\n",
        "    Args:\n",
        "      small_transformer: an instance of the Gemma transformer.\n",
        "      large_transformer: an instance of the Gemma transformer.\n",
        "      tokenizer: tokenizer of the given model.\n",
        "      small_params: weights of the small model.\n",
        "      large_params: weights of the large model.\n",
        "      acceptance_prob_fn: Acceptance criterion function.\n",
        "      residual_distribution_fn: Residual distribution function.\n",
        "      temperature: Temperature for sampling.\n",
        "      cache_length: Length of the cache.\n",
        "    \"\"\"\n",
        "    self.small_transformer = small_transformer\n",
        "    self.large_transformer = large_transformer\n",
        "    self.tokenizer = tokenizer\n",
        "    self.small_params = small_params\n",
        "    self.large_params = large_params\n",
        "    self._compiled_sample_fn = jax.jit(self._sample_fn)\n",
        "    self.acceptance_prob_fn = acceptance_prob_fn\n",
        "    self.residual_distribution_fn = residual_distribution_fn\n",
        "    self.temperature = temperature\n",
        "    self.cache_length = cache_length\n",
        "\n",
        "  @property\n",
        "  def dtype(self) -> jnp.dtype:\n",
        "    # assumes, dtype of both small and large models are same.\n",
        "    return jax.tree_util.tree_leaves(self.small_params)[0].dtype\n",
        "\n",
        "  def _sample_step(\n",
        "      self, small_params, large_params, sampler_state: _SamplingState\n",
        "  ) -> _SamplingState:\n",
        "    \"\"\"Performs a single sampling step.\"\"\"\n",
        "    batch_size = sampler_state.token_buffer.shape[0]\n",
        "    decoding_step = jnp.asarray(sampler_state.decoding_step, dtype=jnp.int32)\n",
        "    last_token = sampler_state.token_buffer[:, decoding_step]\n",
        "    input_mask = sampler_state.token_buffer == self.tokenizer.special_tokens.PAD\n",
        "    # assumes that the cache length is same for both small and large models.\n",
        "    attention_mask = _compute_attention_masks(\n",
        "        decoding_step, self.cache_length, input_mask\n",
        "    )\n",
        "    step_positions = jnp.expand_dims(\n",
        "        sampler_state.positions[:, decoding_step], -1\n",
        "    )\n",
        "    last_token = last_token.reshape((batch_size, 1))\n",
        "\n",
        "    small_out = self.small_transformer.apply(\n",
        "        {'params': small_params},\n",
        "        last_token,\n",
        "        positions=step_positions,\n",
        "        cache=sampler_state.small_cache,\n",
        "        attention_mask=attention_mask,\n",
        "    )\n",
        "    small_cache = small_out.cache\n",
        "    small_logits = small_out.logits\n",
        "\n",
        "    large_out = self.large_transformer.apply(\n",
        "        {'params': large_params},\n",
        "        last_token,\n",
        "        positions=step_positions,\n",
        "        cache=sampler_state.large_cache,\n",
        "        attention_mask=attention_mask,\n",
        "    )\n",
        "    large_cache = large_out.cache\n",
        "    large_logits = large_out.logits\n",
        "\n",
        "    if sampler_state.forbidden_token_ids:\n",
        "      small_logits = small_logits.at[:, :, sampler_state.forbidden_token_ids].set(-jnp.inf)\n",
        "      large_logits = large_logits.at[:, :, sampler_state.forbidden_token_ids].set(-jnp.inf)\n",
        "\n",
        "    next_token_candidate, rng, is_token_accepted = sample_next_token(\n",
        "        small_logits,\n",
        "        large_logits,\n",
        "        self.acceptance_prob_fn,\n",
        "        self.residual_distribution_fn,\n",
        "        sampler_state.rng,\n",
        "        self.temperature)\n",
        "    next_token_candidate = next_token_candidate[:, 0]  # [B,]\n",
        "\n",
        "    next_token_candidate = jnp.where(\n",
        "        decoding_step < sampler_state.num_input_tokens - 1,\n",
        "        sampler_state.token_buffer[:, decoding_step + 1],\n",
        "        next_token_candidate,\n",
        "    )\n",
        "\n",
        "    token_buffer = sampler_state.token_buffer.at[:, decoding_step + 1].set(\n",
        "        next_token_candidate\n",
        "    )\n",
        "\n",
        "    if sampler_state.logits_buffer is not None:\n",
        "      next_logits = jnp.squeeze(large_logits, 1)\n",
        "      logits_buffer = sampler_state.logits_buffer.at[:, decoding_step + 1].set(\n",
        "          next_logits\n",
        "      )\n",
        "    else:\n",
        "      logits_buffer = sampler_state.logits_buffer\n",
        "\n",
        "    done = sampler_state.done | jnp.equal(\n",
        "        token_buffer[:, decoding_step + 1], self.tokenizer.special_tokens.EOS\n",
        "    )\n",
        "\n",
        "    tokens_accepted = sampler_state.tokens_accepted.at[:, decoding_step + 1].set(\n",
        "        jnp.squeeze(is_token_accepted, axis=-1)\n",
        "    )\n",
        "    state = _SamplingState(\n",
        "        decoding_step=sampler_state.decoding_step + 1,\n",
        "        num_input_tokens=sampler_state.num_input_tokens,\n",
        "        token_buffer=token_buffer,\n",
        "        positions=sampler_state.positions,\n",
        "        logits_buffer=logits_buffer,\n",
        "        small_cache=small_cache,\n",
        "        large_cache=large_cache,\n",
        "        done=done,\n",
        "        total_sampling_steps=sampler_state.total_sampling_steps,\n",
        "        tokens_accepted=tokens_accepted,\n",
        "        forbidden_token_ids=sampler_state.forbidden_token_ids,\n",
        "        rng=rng,\n",
        "    )\n",
        "    return state\n",
        "\n",
        "  def init_small_cache(self, bsz) -> dict[str, modules.LayerCache]:\n",
        "    \"\"\"Initializes the attention cache for each layer.\"\"\"\n",
        "    return self.small_transformer.config.init_cache(\n",
        "        batch_size=bsz,\n",
        "        dtype=self.dtype,\n",
        "        cache_length=self.cache_length,\n",
        "    )\n",
        "\n",
        "  def init_large_cache(self, bsz) -> dict[str, modules.LayerCache]:\n",
        "    \"\"\"Initializes the attention cache for each layer.\"\"\"\n",
        "    return self.large_transformer.config.init_cache(\n",
        "        batch_size=bsz,\n",
        "        dtype=self.dtype,\n",
        "        cache_length=self.cache_length,\n",
        "    )\n",
        "\n",
        "  def init_sample_state(\n",
        "      self,\n",
        "      all_input_ids: list[jax.Array],\n",
        "      total_sampling_steps: int,\n",
        "      include_logits: bool = False,\n",
        "      forbidden_token_ids: Sequence[int] | None = None,\n",
        "      rng: jax.random.PRNGKey = jax.random.PRNGKey(0),\n",
        "  ) -> _SamplingState:\n",
        "    \"\"\"Initializes the sampling state given input prompts.\"\"\"\n",
        "    bsz = len(all_input_ids)\n",
        "    num_input_tokens = [len(input_ids) for input_ids in all_input_ids]\n",
        "    buffer_size = total_sampling_steps + 1\n",
        "\n",
        "    token_buffer = jnp.full(\n",
        "        (\n",
        "            bsz,\n",
        "            buffer_size,\n",
        "        ),\n",
        "        self.tokenizer.special_tokens.PAD,\n",
        "        dtype=jnp.int32,\n",
        "    )\n",
        "    input_mask = jnp.ones_like(token_buffer, dtype=jnp.bool_)\n",
        "    for i, (input_ids, num_tokens) in enumerate(\n",
        "        zip(all_input_ids, num_input_tokens)\n",
        "    ):\n",
        "      token_buffer = token_buffer.at[i, :num_tokens].set(input_ids)\n",
        "      input_mask = input_mask.at[i, :num_tokens].set(\n",
        "          input_ids != self.tokenizer.special_tokens.PAD\n",
        "      )\n",
        "    positions = transformer_lib.build_positions_from_mask(input_mask)\n",
        "\n",
        "    done = jnp.zeros((bsz,), dtype=jnp.bool_)\n",
        "\n",
        "    tokens_accepted = jnp.zeros_like(token_buffer, dtype=jnp.bool_)\n",
        "\n",
        "    if include_logits:\n",
        "      logits_buffer = jnp.zeros(\n",
        "          (bsz, buffer_size, self.small_transformer.config.num_embed),\n",
        "          dtype=jnp.float32,\n",
        "      )\n",
        "    else:\n",
        "      logits_buffer = None\n",
        "\n",
        "    return _SamplingState(\n",
        "        decoding_step=0,\n",
        "        num_input_tokens=jnp.array(num_input_tokens, dtype=jnp.int32),\n",
        "        token_buffer=token_buffer,\n",
        "        positions=positions,\n",
        "        logits_buffer=logits_buffer,\n",
        "        small_cache=self.init_small_cache(bsz),\n",
        "        large_cache=self.init_large_cache(bsz),\n",
        "        done=done,\n",
        "        total_sampling_steps=total_sampling_steps,\n",
        "        tokens_accepted=tokens_accepted,\n",
        "        forbidden_token_ids=forbidden_token_ids,\n",
        "        rng=rng,\n",
        "    )\n",
        "\n",
        "  def tokenize(self, input_string: str) -> jax.Array:\n",
        "    \"\"\"Tokenizes the input string.\"\"\"\n",
        "    input_ids = self.tokenizer.encode(input_string)\n",
        "    input_ids = jnp.array(\n",
        "        [self.tokenizer.special_tokens.BOS] + jnp.array(input_ids).tolist(), dtype=jnp.int32\n",
        "    )\n",
        "    return input_ids\n",
        "\n",
        "  def mask_tokens_after_eos_ids(self, token_buffer):\n",
        "    \"\"\"Mask token IDs after the EOS token with the padding ID.\"\"\"\n",
        "    eos_id = self.tokenizer.special_tokens.EOS\n",
        "    eos_exists = jnp.any(jnp.equal(token_buffer, eos_id), axis=-1)\n",
        "    eos_indices = jnp.where(\n",
        "        eos_exists,\n",
        "        jnp.argmax(jnp.equal(token_buffer, eos_id), axis=-1),\n",
        "        token_buffer.shape[-1],\n",
        "    )\n",
        "    mask = jnp.less_equal(\n",
        "        jnp.arange(token_buffer.shape[-1]), eos_indices[:, None]\n",
        "    )\n",
        "    masked_token_buffer = token_buffer * mask + self.tokenizer.special_tokens.PAD * (1 - mask)\n",
        "\n",
        "    return masked_token_buffer\n",
        "\n",
        "  def _sample_fn(\n",
        "      self,\n",
        "      small_params: params_lib.Params,\n",
        "      large_params: params_lib.Params,\n",
        "      initial_sampling_state: _SamplingState,\n",
        "  ) -> _SamplingState:\n",
        "    \"\"\"Internal sampling function (to be jitted).\"\"\"\n",
        "\n",
        "    def sample_with_params(sampler_state: _SamplingState):\n",
        "      return self._sample_step(small_params, large_params, sampler_state)\n",
        "\n",
        "    def cond_fn(sampler_state: _SamplingState):\n",
        "      return (\n",
        "          sampler_state.decoding_step < sampler_state.total_sampling_steps\n",
        "      ) & jnp.any(jnp.logical_not(sampler_state.done))\n",
        "\n",
        "    return jax.lax.while_loop(\n",
        "        cond_fn, sample_with_params, initial_sampling_state\n",
        "    )\n",
        "\n",
        "  def __call__(\n",
        "      self,\n",
        "      input_strings: Sequence[str],\n",
        "      total_generation_steps: int,\n",
        "      echo: bool = False,\n",
        "      return_logits: bool = False,\n",
        "      forbidden_tokens: Sequence[str] | None = None,\n",
        "      seed: int | None = 0,\n",
        "  ) -> SamplerOutput:\n",
        "    \"\"\"Samples a completion of the input string.\n",
        "\n",
        "    Args:\n",
        "      input_strings: input prompts to feed to the model for sampling.\n",
        "      total_generation_steps: number of generation steps. will correspond to the\n",
        "        longest prompt in the batch.\n",
        "      echo: whether to return the prompt as part of the output sample.\n",
        "      return_logits: whether to return per-step logits used during generation.\n",
        "      forbidden_tokens: list of tokens that are forbidden to be generated. Each\n",
        "        token must map to a single token id in the vocab.\n",
        "      seed: random seed.\n",
        "\n",
        "    Returns:\n",
        "      sampler_output: A SamplerOutput object containing the generated samples.\n",
        "    \"\"\"\n",
        "    forbidden_token_ids = None\n",
        "    if forbidden_tokens is not None:\n",
        "      forbidden_token_ids = []\n",
        "      for token in forbidden_tokens:\n",
        "        token_id = self.tokenizer.encode(token)\n",
        "        if len(token_id) != 1:\n",
        "          raise ValueError(\n",
        "              'Forbidden tokens must map to single token ids in the vocab.'\n",
        "          )\n",
        "        forbidden_token_ids.extend(token_id)\n",
        "      forbidden_token_ids = tuple(forbidden_token_ids)\n",
        "    all_input_ids = [self.tokenize(x) for x in input_strings]\n",
        "    max_input_length = max(len(input_ids) for input_ids in all_input_ids)\n",
        "    total_sampling_steps = max_input_length + total_generation_steps\n",
        "    rng = jax.random.PRNGKey(seed)\n",
        "    initial_sampling_state = self.init_sample_state(\n",
        "        all_input_ids,\n",
        "        include_logits=return_logits,\n",
        "        total_sampling_steps=total_sampling_steps,\n",
        "        forbidden_token_ids=forbidden_token_ids,\n",
        "        rng=rng,\n",
        "    )\n",
        "\n",
        "    sampling_state = self._compiled_sample_fn(\n",
        "        self.small_params, self.large_params, initial_sampling_state\n",
        "    )\n",
        "\n",
        "    masked_token_buffer = self.mask_tokens_after_eos_ids(\n",
        "        sampling_state.token_buffer\n",
        "    )\n",
        "\n",
        "    out_tokens = []\n",
        "    out_logits = []\n",
        "    out_accepted = []\n",
        "    for i, (token_buffer, num_tokens) in enumerate(\n",
        "        zip(\n",
        "            masked_token_buffer,\n",
        "            sampling_state.num_input_tokens,\n",
        "        )\n",
        "    ):\n",
        "      start_idx = 0 if echo else num_tokens\n",
        "      out_tokens.append(token_buffer[start_idx:total_sampling_steps].tolist())\n",
        "      token_accepted = sampling_state.tokens_accepted[i]\n",
        "      out_accepted.append(token_accepted[start_idx:total_sampling_steps].tolist())\n",
        "      if return_logits:\n",
        "        logits_buffer = sampling_state.logits_buffer[i]\n",
        "        out_logits.append(\n",
        "            logits_buffer[start_idx:total_sampling_steps].tolist()\n",
        "        )\n",
        "\n",
        "    decoded_outputs = [self.tokenizer.decode(tokens) for tokens in out_tokens]\n",
        "\n",
        "    result = SamplerOutput(\n",
        "        text=decoded_outputs,\n",
        "        logits=out_logits,\n",
        "        tokens=out_tokens,\n",
        "        tokens_accepted=out_accepted,\n",
        "    )\n",
        "    return result"
      ],
      "metadata": {
        "id": "oUzFZhLnyEHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Draft and verify functions\n"
      ],
      "metadata": {
        "id": "An3619sENYCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Elementary draft and verify functions"
      ],
      "metadata": {
        "id": "Ak4ta-iyrsVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accept_all_prob_fn(\n",
        "    probs_small: jnp.ndarray,\n",
        "    probs_large: jnp.ndarray,\n",
        "    probs_small_unscaled: jnp.ndarray,\n",
        "    probs_large_unscaled: jnp.ndarray,\n",
        "    token_small: jnp.ndarray) -> jnp.ndarray:\n",
        "  del probs_small, probs_large, probs_small_unscaled, probs_large_unscaled\n",
        "  return jnp.ones_like(token_small, dtype=jnp.float32)\n",
        "\n",
        "\n",
        "def reject_all_prob_fn(\n",
        "    probs_small: jnp.ndarray,\n",
        "    probs_large: jnp.ndarray,\n",
        "    probs_small_unscaled: jnp.ndarray,\n",
        "    probs_large_unscaled: jnp.ndarray,\n",
        "    token_small: jnp.ndarray) -> jnp.ndarray:\n",
        "  del probs_small, probs_large, probs_small_unscaled, probs_large_unscaled\n",
        "  return jnp.zeros_like(token_small, dtype=jnp.float32)\n",
        "\n",
        "\n",
        "def small_distribution_fn(\n",
        "    probs_small: jnp.ndarray,\n",
        "    probs_large: jnp.ndarray,\n",
        "    probs_small_unscaled: jnp.ndarray,\n",
        "    probs_large_unscaled: jnp.ndarray,\n",
        "    token_small: jnp.ndarray | None = None) -> jnp.ndarray:\n",
        "  del probs_large, probs_small_unscaled, probs_large_unscaled, token_small\n",
        "  return probs_small\n",
        "\n",
        "\n",
        "def large_distribution_fn(\n",
        "    probs_small: jnp.ndarray,\n",
        "    probs_large: jnp.ndarray,\n",
        "    probs_small_unscaled: jnp.ndarray,\n",
        "    probs_large_unscaled: jnp.ndarray,\n",
        "    token_small: jnp.ndarray | None = None) -> jnp.ndarray:\n",
        "  del probs_small, probs_small_unscaled, probs_large_unscaled, token_small\n",
        "  return probs_large"
      ],
      "metadata": {
        "id": "eWGlvzDirnDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lossy speculative decoding: draft & verify functions"
      ],
      "metadata": {
        "id": "9jnDbgDNrydT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def speed_sampling_acceptance_prob_fn(\n",
        "    probs_small: jnp.ndarray,\n",
        "    probs_large: jnp.ndarray,\n",
        "    probs_small_unscaled: jnp.ndarray,\n",
        "    probs_large_unscaled: jnp.ndarray,\n",
        "    token_small: jnp.ndarray,\n",
        "    lenience: float = 0.0) -> jnp.ndarray:\n",
        "  \"\"\"Acceptance function for lossy speculative sampling.\"\"\"\n",
        "  del probs_small_unscaled, probs_large_unscaled\n",
        "  # Small model's probability on token_small.\n",
        "  token_prob_small = jnp.take_along_axis(\n",
        "      probs_small, jnp.expand_dims(token_small, axis=1), axis=-1)  # [B, 1, 1]\n",
        "  token_prob_small = jnp.squeeze(token_prob_small, axis=-1)  # [B, 1]\n",
        "  # Large model's probability on token_small.\n",
        "  token_prob_large = jnp.take_along_axis(\n",
        "      probs_large, jnp.expand_dims(token_small, axis=1), axis=-1)  # [B, 1, 1]\n",
        "  token_prob_large = jnp.squeeze(token_prob_large, axis=-1)  # [B, 1]\n",
        "  # Acceptance probability: min{1, p_large(v) / ((1 - lenience) * p_small(v))}.\n",
        "  # See Leviathan et al., 2023, A.5, Page 12.\n",
        "  denominator = jnp.maximum((1 - lenience) * token_prob_small, MIN_PROBS)\n",
        "  return jnp.minimum(1, token_prob_large / denominator)\n",
        "\n",
        "\n",
        "def speed_sampling_residual_distribution_fn(\n",
        "    probs_small: jnp.ndarray,\n",
        "    probs_large: jnp.ndarray,\n",
        "    probs_small_unscaled: jnp.ndarray,\n",
        "    probs_large_unscaled: jnp.ndarray,\n",
        "    token_small: jnp.ndarray | None = None\n",
        "    ) -> jnp.ndarray:\n",
        "  \"\"\"Residual distribution for lossy speculative sampling.\"\"\"\n",
        "  del probs_small_unscaled, probs_large_unscaled, token_small\n",
        "  # Residual distribution is max{0, p_large(.) - p_small(.)}.\n",
        "  return jnp.maximum(0.0, probs_large - probs_small)"
      ],
      "metadata": {
        "id": "vsZIVK_grxAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speculative cascade: draft & verify functions with generic target distribution\n"
      ],
      "metadata": {
        "id": "CuW0ongRsNP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_speculative_cascade_sampling_acceptance_prob_fn(\n",
        "    target_distribution_fn, lenience=0.0\n",
        "):\n",
        "  \"\"\"Return a function that computes acceptance criteria for a sampling speculative cascade with target_distribution_fn.\"\"\"\n",
        "  def speculative_cascade_sampling_acceptance_prob_fn(\n",
        "      probs_small: jnp.ndarray,\n",
        "      probs_large: jnp.ndarray,\n",
        "      probs_small_unscaled: jnp.ndarray,\n",
        "      probs_large_unscaled: jnp.ndarray,\n",
        "      token_small: jnp.ndarray,\n",
        "  ) -> jnp.ndarray:\n",
        "    probs_target = target_distribution_fn(\n",
        "        probs_small,\n",
        "        probs_large,\n",
        "        probs_small_unscaled,\n",
        "        probs_large_unscaled,\n",
        "        token_small,\n",
        "        lenience\n",
        "    )\n",
        "    # Apply loss-less SPEED (lenience = 0) with the target distribution.\n",
        "    return speed_sampling_acceptance_prob_fn(\n",
        "        probs_small,\n",
        "        probs_target,\n",
        "        probs_small_unscaled,\n",
        "        probs_large_unscaled,\n",
        "        token_small,\n",
        "        lenience=0\n",
        "    )\n",
        "  return speculative_cascade_sampling_acceptance_prob_fn\n",
        "\n",
        "\n",
        "def create_speculative_cascade_sampling_residual_distribution_fn(\n",
        "    target_distribution_fn, lenience=0.0\n",
        "):\n",
        "  \"\"\"Return a function that computes residual distribution for a sampling speculative cascade with target_distribution_fn.\"\"\"\n",
        "  def speculative_cascade_residual_distribution_fn(\n",
        "      probs_small: jnp.ndarray,\n",
        "      probs_large: jnp.ndarray,\n",
        "      probs_small_unscaled: jnp.ndarray,\n",
        "      probs_large_unscaled: jnp.ndarray,\n",
        "      token_small: jnp.ndarray,\n",
        "  ) -> jnp.ndarray:\n",
        "    probs_target = target_distribution_fn(\n",
        "        probs_small,\n",
        "        probs_large,\n",
        "        probs_small_unscaled,\n",
        "        probs_large_unscaled,\n",
        "        token_small,\n",
        "        lenience\n",
        "    )\n",
        "    # Apply loss-less SPEED with the target distribution.\n",
        "    return speed_sampling_residual_distribution_fn(\n",
        "        probs_small,\n",
        "        probs_target,\n",
        "        probs_small_unscaled,\n",
        "        probs_large_unscaled,\n",
        "        token_small\n",
        "    )\n",
        "  return speculative_cascade_residual_distribution_fn"
      ],
      "metadata": {
        "id": "OrswGySEsLDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speculative cascade: target distributions for different deferral rules"
      ],
      "metadata": {
        "id": "f1wzZtTEstIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def target_distribution_chow(\n",
        "    probs_small: jnp.ndarray,\n",
        "    probs_large: jnp.ndarray,\n",
        "    probs_small_unscaled: jnp.ndarray,\n",
        "    probs_large_unscaled: jnp.ndarray,\n",
        "    token_small: jnp.ndarray,\n",
        "    lenience: float = -1.0\n",
        ") -> jnp.ndarray:\n",
        "  \"\"\"Target distribution for Chow deferral rule.\"\"\"\n",
        "  del probs_large_unscaled, token_small\n",
        "  max_prob_small = jnp.max(probs_small_unscaled, axis=-1, keepdims=True)\n",
        "  # Chow criterion: equation 2 in Narasimhan et al. (2025):\n",
        "  #   max_v p_small(v) >= 1 - lenience.\n",
        "  pick_small = jnp.greater_equal(max_prob_small, 1.0 - lenience)\n",
        "  return pick_small * probs_small + (1 - pick_small) * probs_large\n",
        "\n",
        "\n",
        "def target_distribution_diff(\n",
        "    probs_small: jnp.ndarray,\n",
        "    probs_large: jnp.ndarray,\n",
        "    probs_small_unscaled: jnp.ndarray,\n",
        "    probs_large_unscaled: jnp.ndarray,\n",
        "    token_small: jnp.ndarray,\n",
        "    lenience: float = -1.0\n",
        ") -> jnp.ndarray:\n",
        "  \"\"\"Target distribution for Diff deferral rule.\"\"\"\n",
        "  del token_small\n",
        "  max_prob_small = jnp.max(probs_small_unscaled, axis=-1, keepdims=True)\n",
        "  max_prob_large = jnp.max(probs_large_unscaled, axis=-1, keepdims=True)\n",
        "  # Diff criterion: equation 5 in Narasimhan et al. (2025):\n",
        "  #   max_v p_small(v) >= max_v p_large(v) - lenience.\n",
        "  pick_small = jnp.greater_equal(max_prob_small, max_prob_large - lenience)\n",
        "  return pick_small * probs_small + (1 - pick_small) * probs_large\n",
        "\n",
        "\n",
        "def target_distribution_opt(\n",
        "    probs_small: jnp.ndarray,\n",
        "    probs_large: jnp.ndarray,\n",
        "    probs_small_unscaled: jnp.ndarray,\n",
        "    probs_large_unscaled: jnp.ndarray,\n",
        "    token_small: jnp.ndarray,\n",
        "    lenience: float = -1.0) -> jnp.ndarray:\n",
        "  \"\"\"Target distribution for OPT deferral rule.\"\"\"\n",
        "  del token_small\n",
        "  max_prob_small = jnp.max(probs_small_unscaled, axis=-1, keepdims=True)\n",
        "  max_prob_large = jnp.max(probs_large_unscaled, axis=-1, keepdims=True)\n",
        "  # OPT criterion: equation 10 in Narasimhan et al. (2025):\n",
        "  #   max_v p_small(v) >= max_v p_large(v) - lenience * TVD(p_small, p_large),\n",
        "  #      where TV-distance between p_small and p_large is defined as:\n",
        "  #           \\sum_v max{0, p_small(v) - p_large(v)}.\n",
        "  tvd = jnp.sum(\n",
        "      jnp.maximum(0.0, probs_small - probs_large), axis=-1, keepdims=True\n",
        "  )\n",
        "  pick_small = jnp.greater_equal(\n",
        "      max_prob_small, max_prob_large - lenience * tvd)\n",
        "  return pick_small * probs_small + (1 - pick_small) * probs_large\n",
        "\n",
        "\n",
        "def target_distribution_token_v1(\n",
        "    probs_small: jnp.ndarray,\n",
        "    probs_large: jnp.ndarray,\n",
        "    probs_small_unscaled: jnp.ndarray,\n",
        "    probs_large_unscaled: jnp.ndarray,\n",
        "    token_small: jnp.ndarray,\n",
        "    lenience: float = -1.0) -> jnp.ndarray:\n",
        "  \"\"\"Target distribution for Token-v1 deferral rule.\"\"\"\n",
        "  del token_small\n",
        "  max_prob_large = jnp.max(\n",
        "      probs_large_unscaled, axis=-1, keepdims=True)  # [B, 1, 1]\n",
        "  # Token-v1 criterion: equation 13 in Narasimhan et al. (2025):\n",
        "  #   p_small_unscaled(v) >= max_u p_large_unscaled(u) - lenience.\n",
        "  tokens_accepted = jnp.greater_equal(\n",
        "      probs_small_unscaled, max_prob_large - lenience)  # [B, 1, V]\n",
        "  # p_res(v) = p_small(v) * 1(v accepted) +\n",
        "  #               (1 - \\sum_u p_small(u) * 1(u rejected)) * p_large(v).\n",
        "  probs_small_accepted = probs_small * tokens_accepted\n",
        "  probs_small_accepted_sum = 1.0 - jnp.sum(\n",
        "      probs_small_accepted, axis=-1, keepdims=True)\n",
        "  return probs_small_accepted + probs_small_accepted_sum * probs_large\n",
        "\n",
        "\n",
        "def target_distribution_token_v2(\n",
        "    probs_small: jnp.ndarray,\n",
        "    probs_large: jnp.ndarray,\n",
        "    probs_small_unscaled: jnp.ndarray,\n",
        "    probs_large_unscaled: jnp.ndarray,\n",
        "    token_small: jnp.ndarray,\n",
        "    lenience: float = -1.0) -> jnp.ndarray:\n",
        "  \"\"\"Target distribution for Token-v2 deferral rule.\"\"\"\n",
        "  del probs_small_unscaled, token_small\n",
        "  token_prob_large = jnp.max(\n",
        "      probs_large_unscaled, axis=-1, keepdims=True)  # [B, 1, 1]\n",
        "  # Token-v2 criterion: equation 14 in Narasimhan et al. (2025):\n",
        "  #   probs_large_unscaled(v) >= max_u probs_large_unscaled(u) - lenience.\n",
        "  tokens_accepted = jnp.greater_equal(\n",
        "      probs_large_unscaled, token_prob_large - lenience)  # [B, 1, V]\n",
        "  # p_res(v) = p_small(v) * 1(v accepted) +\n",
        "  #               (1 - \\sum_u p_small(u) * 1(u rejected)) * p_large(v).\n",
        "  probs_small_accepted = probs_small * tokens_accepted\n",
        "  probs_small_accepted_sum = 1.0 - jnp.sum(\n",
        "      probs_small_accepted, axis=-1, keepdims=True)\n",
        "  return probs_small_accepted + probs_small_accepted_sum * probs_large\n",
        "\n",
        "\n",
        "def target_distribution_token_v3(\n",
        "    probs_small: jnp.ndarray,\n",
        "    probs_large: jnp.ndarray,\n",
        "    probs_small_unscaled: jnp.ndarray,\n",
        "    probs_large_unscaled: jnp.ndarray,\n",
        "    token_small: jnp.ndarray,\n",
        "    lenience: float = 0.0) -> jnp.ndarray:\n",
        "  \"\"\"Target distribution for Token-v3 deferral rule.\"\"\"\n",
        "  del probs_small_unscaled, token_small\n",
        "  token_prob_large = jnp.max(\n",
        "      probs_large_unscaled, axis=-1, keepdims=True)  # [B, 1, 1]\n",
        "  # Token-v3 criterion: equation 15 in Narasimhan et al. (2025):\n",
        "  #   p_large_unscaled(v) >= (1 - lenience) max_u p_large_unscaled(u).\n",
        "  tokens_accepted = jnp.greater_equal(\n",
        "      probs_large_unscaled, token_prob_large * (1 - lenience))  # [B, 1, V]\n",
        "  # p_res(v) = p_small(v) * 1(v accepted) +\n",
        "  #               (1 - \\sum_u p_small(u) * 1(u rejected)) * p_large(v).\n",
        "  probs_small_accepted = probs_small * tokens_accepted\n",
        "  probs_small_rejected_sum = 1.0 - jnp.sum(\n",
        "      probs_small_accepted, axis=-1, keepdims=True)\n",
        "  return probs_small_accepted + probs_small_rejected_sum * probs_large"
      ],
      "metadata": {
        "id": "yqF6vCU1srOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get acceptance and residual functions for different methods"
      ],
      "metadata": {
        "id": "y8Ty_Tl0tTCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_acceptance_residual_fns(method: str, lenience: float = 0.0):\n",
        "  \"\"\"Returns (acceptance_fn, residual_fn) for the selected method.\n",
        "\n",
        "  Args:\n",
        "    method: can be 'drafter_only', 'verifier_only', 'speed',\n",
        "      'cascade_chow', 'cascade_diff', 'cascade_opt', 'cascade_tokenV1',\n",
        "      'cascade_tokenV2', or 'cascade_tokenV3'.\n",
        "    lenience: Lenience parameter alpha for deferral rule.\n",
        "  \"\"\"\n",
        "  if method == 'drafter_only':\n",
        "    return accept_all_prob_fn, small_distribution_fn\n",
        "  elif method == 'verifier_only':\n",
        "    return reject_all_prob_fn, large_distribution_fn\n",
        "  elif method == 'speed':\n",
        "    speed_acceptance_prob_fn = (\n",
        "        lambda x, y, u, v, w: speed_sampling_acceptance_prob_fn(\n",
        "            x, y, u, v, w, lenience=lenience\n",
        "        )\n",
        "    )\n",
        "    return speed_acceptance_prob_fn, speed_sampling_residual_distribution_fn\n",
        "  elif method.startswith('cascade'):\n",
        "    # Syntax: cascade_<deferral_rule>\n",
        "    method_splits = method.split('_')\n",
        "    if len(method_splits) != 2:\n",
        "      raise ValueError(f'Invalid method syntax: {method}')\n",
        "    deferral_rule = method_splits[1]\n",
        "    if deferral_rule == 'chow':\n",
        "      target_distribution_fn = target_distribution_chow\n",
        "    elif deferral_rule == 'diff':\n",
        "      target_distribution_fn = target_distribution_diff\n",
        "    elif deferral_rule == 'opt':\n",
        "      target_distribution_fn = target_distribution_opt\n",
        "    elif deferral_rule == 'tokenV1':\n",
        "      target_distribution_fn = target_distribution_token_v1\n",
        "    elif deferral_rule == 'tokenV2':\n",
        "      target_distribution_fn = target_distribution_token_v2\n",
        "    elif deferral_rule == 'tokenV3':\n",
        "      target_distribution_fn = target_distribution_token_v3\n",
        "    else:\n",
        "      raise ValueError(f'Unknown deferral rule: {deferral_rule}')\n",
        "    spec_cascade_acceptance_prob_fn = (\n",
        "        create_speculative_cascade_sampling_acceptance_prob_fn(\n",
        "            target_distribution_fn, lenience\n",
        "        )\n",
        "    )\n",
        "    spec_cascade_residual_distribution_fn = (\n",
        "        create_speculative_cascade_sampling_residual_distribution_fn(\n",
        "            target_distribution_fn, lenience\n",
        "        )\n",
        "    )\n",
        "    return (\n",
        "        spec_cascade_acceptance_prob_fn,\n",
        "        spec_cascade_residual_distribution_fn,\n",
        "    )\n",
        "  else:\n",
        "    raise ValueError(f'Unknown method: {method}')"
      ],
      "metadata": {
        "id": "ARNeXpM9tRcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzEj3PYvX1Sm"
      },
      "source": [
        "# Load a Gemma 2B and 9B model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = gm.text.Gemma2Tokenizer()\n",
        "\n",
        "# Load the Transformer model\n",
        "small_transformer = gm.nn.Gemma2_2B()\n",
        "large_transformer = gm.nn.Gemma2_9B()\n",
        "\n",
        "# Load the params.\n",
        "small_params = gm.ckpts.load_params(gm.ckpts.CheckpointPath.GEMMA2_2B_IT)\n",
        "large_params = gm.ckpts.load_params(gm.ckpts.CheckpointPath.GEMMA2_9B_IT)"
      ],
      "metadata": {
        "id": "VorsDEqRSsYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test samplers with example prompt\n",
        "\n"
      ],
      "metadata": {
        "id": "41juhJjIUsy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a sampler on top of your model and your tokenizer, and test it on an\n",
        "\n",
        "def acceptance_rate(out_data, idx=0):\n",
        "  all_tokens = out_data.tokens[idx]\n",
        "  all_accepted = out_data.tokens_accepted[idx]\n",
        "  eos_idx = np.where(np.array(all_tokens) == 1)[0]\n",
        "  if len(eos_idx) == 0:\n",
        "    # No EOS. So num_gen_steps not enough.\n",
        "    return np.sum(all_accepted) / len(all_tokens)\n",
        "  else:\n",
        "    eos_idx = eos_idx[0]\n",
        "    if eos_idx == 0:\n",
        "      # EOS in first position.\n",
        "      return 1.0\n",
        "    return np.sum(all_accepted[:eos_idx]) / eos_idx\n",
        "\n",
        "\n",
        "def test_sample(\n",
        "    acceptance_prob_fn,\n",
        "    residual_distribution_fn,\n",
        "    prompts,\n",
        "    temperature=1.0,\n",
        "    total_generation_steps=200,\n",
        "    verbose=True,\n",
        "    seed=0):\n",
        "  if not isinstance(prompts, list):\n",
        "    prompts = [prompts]\n",
        "  # Build sampler.\n",
        "  sampler = Sampler(\n",
        "    small_transformer=small_transformer,\n",
        "    large_transformer=large_transformer,\n",
        "    tokenizer=tokenizer,\n",
        "    small_params=small_params,\n",
        "    large_params=large_params,\n",
        "    acceptance_prob_fn=acceptance_prob_fn,\n",
        "    residual_distribution_fn=residual_distribution_fn,\n",
        "    temperature=temperature\n",
        "  )\n",
        "  # Response for example prompt.\n",
        "  out_data = sampler(\n",
        "      input_strings=prompts,\n",
        "      total_generation_steps=total_generation_steps,\n",
        "      seed=seed)\n",
        "  if verbose:\n",
        "    print(f\"Response:\\n{out_data.text[0]}\\n\")\n",
        "    print(f\"Acceptance rate:\\n{acceptance_rate(out_data)}\")\n",
        "  return out_data"
      ],
      "metadata": {
        "id": "HcHFFqyTUq35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unless otherwise specified, we sample with temperature 1.0"
      ],
      "metadata": {
        "id": "ltGfvts6VhS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"what's the purpose of life?\""
      ],
      "metadata": {
        "id": "vYpL2x1SeKAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may pick one of the following methods:\n",
        "\n",
        "- **drafter_only**: Always call the drafter\n",
        "- **verifier_only**: Always call the verifier\n",
        "- **speed**: Lossy SPEED, where lenience parameter can vary from 0 (standard SPEED) to 1 (only drafter)\n",
        "- **cascade_chow:** Speculative cascade with Chow's rule, where lenience can vary from 0 (standard SPEED) to 1 (only drafter)\n",
        "- **cascade_diff:** Speculative cascade with the Diff rule, where lenience can vary from -1 (standard SPEED) to 1 (only drafter)\n",
        "- **cascade_opt:** Speculative cascade with the OPT rule, where lenience can vary from -Inf (standard SPEED) to Inf (only drafter)\n",
        "- **cascade_tokenV1:** Speculative cascade with the TokenV1 rule, where lenience can vary from -1 (standard SPEED) to 1 (only drafter)\n",
        "- **cascade_tokenV2:** Speculative cascade with the TokenV2 rule, where lenience can vary from -1 (standard SPEED) to 1 (only drafter)\n",
        "- **cascade_tokenV3:** Speculative cascade with the TokenV3 rule, where lenience can vary from 0 (standard SPEED) to 1 (only drafter)"
      ],
      "metadata": {
        "id": "pzr4yaz9utLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample response for prompt.\n",
        "\n",
        "# Pick one among the different speculative cascade deferral rules and the baselines.\n",
        "method = 'cascade_tokenV3'  # @param ['drafter_only', 'verifier_only', 'speed', 'cascade_chow', 'cascade_diff', 'cascade_opt', 'cascade_tokenV1', 'cascade_tokenV2', 'cascade_tokenV3']\n",
        "\n",
        "# Temperature for sampling.\n",
        "temperature = 1.0  # @param {type:'number'}\n",
        "\n",
        "# The lenience parameter is same as the trade-off parameter `alpha` in the paper.\n",
        "lenience = 0.5  # @param {type:'number'}\n",
        "\n",
        "# The number of output tokens is strictly bounded by this parameter.\n",
        "total_generation_steps = 100  # @param {type:'integer'}\n",
        "\n",
        "\n",
        "acceptance_fn, residual_fn = get_acceptance_residual_fns(\n",
        "    method=method,\n",
        "    lenience=lenience\n",
        ")\n",
        "\n",
        "out_data = test_sample(\n",
        "    acceptance_fn,\n",
        "    residual_fn,\n",
        "    prompts=prompt,\n",
        "    temperature=temperature,\n",
        "    total_generation_steps=total_generation_steps\n",
        ")"
      ],
      "metadata": {
        "id": "lkljScWCtNwK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "QyhGYKBaoOgm",
        "iwQuY2tOoSzD",
        "Ak4ta-iyrsVV",
        "9jnDbgDNrydT",
        "CuW0ongRsNP7",
        "f1wzZtTEstIC",
        "y8Ty_Tl0tTCk",
        "XzEj3PYvX1Sm"
      ],
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3",
        "kind": "private"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}