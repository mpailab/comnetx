{
  "cells": [
    {
      "metadata": {
        "id": "LEifDO_tXfX0"
      },
      "cell_type": "markdown",
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "prU7FAXAA8q8",
        "jupyter": {
          "source_hidden": true
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# @title Notebook setup.\n",
        "%cd ..\n",
        "import random\n",
        "from semantic_routing.benchmark import utils\n",
        "from semantic_routing.benchmark.query_engines import labeled_query_engines\n",
        "from semantic_routing.benchmark.query_engines import basic_query_engines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxejbSVWeks0"
      },
      "source": [
        "# Query Engines\n",
        "\n",
        "In order to simulate ground-truth optimal routes in a programmatic fashion, every entry in the user query dataset is labeled with a ground-truth structured interpretation of their content.\n",
        "Below, we provide examples of user text queries that can be found in, or generated by, the user query dataset.\n",
        "\n",
        "These examples are sampled by calling `engine.sample_query(split, rng)` on an instantiated query engine `engine`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "e8vX115rAWOX",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# @title Examples of user queries from dataset.\n",
        "\n",
        "poi_specs = utils.get_poi_specs()\n",
        "rng = random.Random(0)\n",
        "\n",
        "num_queries_to_generate = 5 # @param\n",
        "\n",
        "engine = labeled_query_engines.HumanLabeledQueryEngine(poi_specs=poi_specs, splits=(0.95, 0, 0.05), seed=0)\n",
        "engine.touring_prop = 0\n",
        "print(\"Examples of waypoint routing queries, generated by the query engine...\")\n",
        "for _ in range(num_queries_to_generate):\n",
        "  query_data, query_text = engine.sample_query(0, rng)\n",
        "  print(\"\\\"{}\\\"\".format(query_text.strip()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xMn3I58rnzu"
      },
      "source": [
        "A basic synthetic query engine, which composes user queries using natural language templates, is also available for use and debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "yxE9v3s1rrWB",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# @title Examples of user queries from basic engine.\n",
        "\n",
        "poi_specs = utils.get_poi_specs(benchmark.REDUCED_POI_SPECS_PATH)\n",
        "rng = random.Random(0)\n",
        "\n",
        "num_queries_to_generate = 5 # @param\n",
        "\n",
        "engine = basic_query_engines.POIBasedRoutingQueryEngine(poi_specs=poi_specs, splits=(0.95, 0, 0.05), seed=0)\n",
        "print(\"Examples of waypoint routing queries, generated by the query engine...\")\n",
        "for _ in range(num_queries_to_generate):\n",
        "  query_data, query_text = engine.sample_query(0, rng)\n",
        "  print(\"\\\"{}\\\"\".format(query_text.strip()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDAIAuKlDkAo"
      },
      "source": [
        "## The Content of a Waypoint Routing Query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAEX7Sdnf3s3"
      },
      "source": [
        "A waypoint routing query communicates one of two types of information:\n",
        "\n",
        "1. A \"need\" which can be fulfilled by stopping by a point-of-interest. For example, gasoline is a need which can be fulfilled by stopping at a gas station along the route.\n",
        "\n",
        "2. A \"driving preference\" which should be taken into account along with driving time when weighing the cost of a route. For example, a driving preference is disliking highways. Roughly 60% of queries have no road preferences and 20%/20% like and dislike highways respectively.\n",
        "\n",
        "To programatically evaluate how well a route satisfies a user query, and hence generate ground-truth routes for our dataset, we implement the following criteria.\n",
        "1. If the route does not reach the desired destination, it is immediately disqualified.\n",
        "2. If the route does not stop by POIs that satisfy all of the user's needs, it is immediately disqualified.\n",
        "3. The smaller the route's total travel time the better. If the user happens to prefer/disprefer driving on highways, we multiply the user's travel time on highways by 0.5/5. See `points_of_interest_and_driving_preferences_tutorial.ipynb` for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "ZLCMQ7QIeb-P",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# @title Examples of waypoint routing queries from training and testing splits.\n",
        "\n",
        "poi_specs = utils.get_poi_specs(benchmark.POI_SPECS_PATH)\n",
        "rng = random.Random(0)\n",
        "engine = labeled_query_engines.HumanLabeledQueryEngine(poi_specs=poi_specs, splits=(0.95, 0, 0.05), seed=0)\n",
        "i = 0\n",
        "while i \u003c 5:\n",
        "  query_data, query_text = engine.sample_query(0, rng)\n",
        "  if \"time_budget\" not in query_data:\n",
        "    print(\"From training split: \\\"{}\\\"\".format(query_text.strip()))\n",
        "    i += 1\n",
        "\n",
        "i = 0\n",
        "while i \u003c 5:\n",
        "  query_data, query_text = engine.sample_query(2, rng)\n",
        "  if \"time_budget\" not in query_data:\n",
        "    print(\"From testing split: \\\"{}\\\"\".format(query_text.strip()))\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "FmLREhMthK_G",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# @title A sample of waypoint routing query statistics.\n",
        "\n",
        "rng = random.Random(0)\n",
        "road_preferences_count = {k: 0 for k in (\"like highways\", \"dislike highways\")}\n",
        "poi_size_count = {k: 0 for k in range(8)}\n",
        "for _ in range(100):\n",
        "  query_data, query_text = engine.sample_query(0, rng)\n",
        "  poi_size_count[len(query_data[\"pois\"])] += 1\n",
        "  if query_data[\"linear\"]:\n",
        "    road_preferences_count[query_data[\"linear\"]] += 1\n",
        "print(\"Training set road preference counts (of 100): \", road_preferences_count)\n",
        "print(\"Training set POI request size counts (of 100): \", poi_size_count)\n",
        "road_preferences_count = {k: 0 for k in (\"like highways\", \"dislike highways\")}\n",
        "poi_size_count = {k: 0 for k in range(8)}\n",
        "for _ in range(100):\n",
        "  query_data, query_text = engine.sample_query(2, rng)\n",
        "  poi_size_count[len(query_data[\"pois\"])] += 1\n",
        "  if query_data[\"linear\"]:\n",
        "    road_preferences_count[query_data[\"linear\"]] += 1\n",
        "print(\"Training set road preference counts (of 100): \", road_preferences_count)\n",
        "print(\"Training set POI request size counts (of 100): \", poi_size_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoKNWoy8Fm5j"
      },
      "source": [
        "## The Content of a Trip Planning Query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByJdauEBFrG3"
      },
      "source": [
        "A trip planning query communicates one of two types of information:\n",
        "\n",
        "1. A \"desire\" which can be fulfilled by stopping by a point-of-interest. For example, sampling local wines is a desire which can be entertained by visiting a winery.\n",
        "\n",
        "2. A time budget: the total amount of time the user can spend on the road. This is not taking into account time spent at a venue, which is assumed to be at the user's discretion.\n",
        "\n",
        "As with waypoint routing queries, the set of user queries that are found in our dataset's testing split are significantly more challenging than those found in the training split. In the training set, most queries specify at most one desire. In the test set, all queries specify at least two desires.\n",
        "\n",
        "To programatically evaluate how well an itinerary satisfies a user query, and hence generate ground-truth routes for our dataset, we implement the following criteria.\n",
        "1. If it is impossible for the user to complete the itinerary within  the time budget, it is immediately disqualified.\n",
        "2. For every user desire that is satisfied by at least one POI in the itinerary, a reward of +1000 is awarded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "NHNsGU7pGjCo",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# @title Examples of trip planning queries from training and testing splits.\n",
        "\n",
        "rng = random.Random(0)\n",
        "engine = labeled_query_engines.HumanLabeledQueryEngine(poi_specs=poi_specs, splits=(0.95, 0, 0.05), seed=0)\n",
        "engine.touring_prop = 1\n",
        "i = 0\n",
        "while i \u003c 3:\n",
        "  query_data, query_text = engine.sample_query(0, rng)\n",
        "  if \"time_budget\" in query_data:\n",
        "    print(\"From training split: \\\"{}\\\"\".format(query_text.strip()))\n",
        "    i += 1\n",
        "\n",
        "i = 0\n",
        "while i \u003c 3:\n",
        "  query_data, query_text = engine.sample_query(2, rng)\n",
        "  if \"time_budget\" in query_data:\n",
        "    print(\"From testing split: \\\"{}\\\"\".format(query_text.strip()))\n",
        "    i += 1"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
